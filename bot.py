import os
import streamlit as st
import datasets
from functools import partial
from loguru import logger
from utils import (
    generate_together_stream,
    generate_with_references,
    DEBUG,
)
from datasets.utils.logging import disable_progress_bar

disable_progress_bar()

# Set page configuration
st.set_page_config(page_title="Together AI MoA Chatbot", page_icon="ü§ñ", layout="wide")

# Custom CSS
st.markdown("""
    <style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .st-bw {
        background-color: #f0f2f6;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
    }
    .stTextInput>div>div>input {
        background-color: #ffffff;
        color: #000000;
    }
    .stChatInputContainer {
        padding-bottom: 20px;
    }
    .stChatInputContainer > div {
        background-color: #ffffff;
        border-radius: 20px;
        border: 1px solid #e0e0e0;
    }
    .stChatInputContainer input {
        color: #000000 !important;
    }
    input, textarea {
        color: #000000 !important;
    }
    </style>
    """, unsafe_allow_html=True)

# Welcome message
welcome_message = """
# MoA (Mixture-of-Agents) Chatbot

Ph∆∞∆°ng ph√°p Mixture of Agents (MoA) l√† m·ªôt k·ªπ thu·∫≠t m·ªõi, t·ªï ch·ª©c nhi·ªÅu m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) th√†nh m·ªôt ki·∫øn tr√∫c nhi·ªÅu l·ªõp. M·ªói l·ªõp bao g·ªìm nhi·ªÅu "t√°c nh√¢n" (m√¥ h√¨nh LLM ri√™ng l·∫ª). C√°c t√°c nh√¢n n√†y h·ª£p t√°c v·ªõi nhau b·∫±ng c√°ch t·∫°o ra c√°c ph·∫£n h·ªìi d·ª±a tr√™n ƒë·∫ßu ra t·ª´ c√°c t√°c nh√¢n ·ªü l·ªõp tr∆∞·ªõc, t·ª´ng b∆∞·ªõc tinh ch·ªânh v√† c·∫£i thi·ªán k·∫øt qu·∫£ cu·ªëi c√πng, ch·ªâ s·ª≠ d·ª•ng c√°c m√¥ h√¨nh m√£ ngu·ªìn m·ªü(Open-source)!

Truy c·∫≠p B√†i nghi√™n c·ª©u g·ªëc ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt: [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692)

Chatbot n√†y s·ª≠ d·ª•ng c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) sau ƒë√¢y l√†m c√°c l·ªõp ‚Äì M√¥ h√¨nh tham chi·∫øu, sau ƒë√≥ chuy·ªÉn k·∫øt qu·∫£ cho m√¥ h√¨nh t·ªïng h·ª£p ƒë·ªÉ t·∫°o ra ph·∫£n h·ªìi cu·ªëi c√πng:
"""

default_reference_models = [
    "Qwen/Qwen2-72B-Instruct",
    "Qwen/Qwen1.5-110B-Chat",
    "meta-llama/Llama-3-70b-chat-hf",
    "microsoft/WizardLM-2-8x22B",
    "databricks/dbrx-instruct",
]

def read_file_content(filename):
    current_dir = os.path.dirname(__file__)
    file_path = os.path.join(current_dir, filename)
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    return f"Error: File {filename} not found."

license_content = read_file_content("LICENSE")
notice_content = read_file_content("NOTICE")

def process_fn(item, temperature=0.5, max_tokens=2048):
    references = item.get("references", [])
    model = item["model"]
    messages = item["instruction"]

    output = generate_with_references(
        model=model,
        messages=messages,
        references=references,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    if DEBUG:
        logger.info(
            f"model: {model}, instruction: {item['instruction']}, output: {output[:20]}"
        )

    st.write(f"Finished querying **{model}**.")

    return {"output": output}

def show_legal_info():
    st.title("Legal Information")
    
    st.header("NOTICE")
    st.text(notice_content)
    
    st.header("LICENSE")
    st.text(license_content)
    
    if st.button("Back to Chat"):
        st.session_state.show_legal_info = False
        st.experimental_rerun()

def main():
    if "show_legal_info" not in st.session_state:
        st.session_state.show_legal_info = False
    
    if st.session_state.show_legal_info:
        show_legal_info()
    else:
        st.markdown(welcome_message, unsafe_allow_html=True)
        
        # Display reference models
        col1, col2 = st.columns(2)
        for i, model in enumerate(default_reference_models):
            if i < len(default_reference_models) // 2:
                col1.markdown(f"- {model}")
            else:
                col2.markdown(f"- {model}")

        st.markdown("---")

        # Sidebar for configuration
        st.sidebar.header("Configuration")
        
        unique_models = list(dict.fromkeys(["Qwen/Qwen2-72B-Instruct"] + default_reference_models))
        
        model = st.sidebar.selectbox(
            "Main model",
            unique_models,
            index=0
        )
        temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.5)
        max_tokens = st.sidebar.slider("Max tokens", 1, 4096, 2048)

        # Add legal information button to sidebar
        if st.sidebar.button("Legal Information"):
            st.session_state.show_legal_info = True
            st.experimental_rerun()

        # Chat interface
        st.header("üí¨ Chat with MoA")
        
        # Initialize chat history
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Display chat messages from history on app rerun
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # React to user input
        if prompt := st.chat_input("What would you like to know?"):
            st.chat_message("user").markdown(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})

            # Generate response
            data = {
                "instruction": [[{"role": "user", "content": prompt}] for _ in range(len(default_reference_models))],
                "references": [""] * len(default_reference_models),
                "model": [m for m in default_reference_models],
            }

            eval_set = datasets.Dataset.from_dict(data)

            with st.spinner("Thinking..."):
                for i_round in range(1):
                    eval_set = eval_set.map(
                        partial(
                            process_fn,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        ),
                        batched=False,
                        num_proc=len(default_reference_models),
                    )
                    references = [item["output"] for item in eval_set]
                    data["references"] = references
                    eval_set = datasets.Dataset.from_dict(data)

                st.write("**Aggregating results & querying the aggregate model...**")
                output = generate_with_references(
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    messages=data["instruction"][0],
                    references=references,
                    generate_fn=generate_together_stream,
                )

                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    full_response = ""
                    for chunk in output:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "‚ñå")
                    message_placeholder.markdown(full_response)
                
                st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
